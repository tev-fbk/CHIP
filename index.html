<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JNN7F7HDFD"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-JNN7F7HDFD');
  </script>
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bulma@1.0.2/css/bulma.min.css"
  >
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        
        <div class="columns is-centered">

          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <a href="https://tev.fbk.eu/team/mattia-nardon" target="_blank">Mattia Nardon</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/mikel-mujika-agirre-a82611239/" target="_blank">Mikel Mujika Agirre</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/agonzat/" target="_blank">Ander González Tomé</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/dsedano92/" target="_blank">Daniel Sedano Algarabel</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/josep-rueda-collell/" target="_blank">Josep Rueda Collell</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/ana-paola-caro-ospina-04ab79186/" target="_blank">Ana Paola Caro</a><sup>3</sup>,
                </span>
                <span class="author-block">
                  <a href="https://tev.fbk.eu/team/andrea-caraffa" target="_blank">Andrea Caraffa</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://fabiopoiesi.github.io/" target="_blank">Fabio Poiesi</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://tev.fbk.eu/team/paul-chippendale" target="_blank">Paul Ian Chippendale</a><sup>1</sup>
                </span>
                <span class="author-block">
                  <a href="https://davideboscaini.github.io/" target="_blank">Davide Boscaini</a><sup>1</sup>
                </span>
                  </div>
  
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="margin-right: 2rem;"><sup>1</sup>Fondazione Bruno Kessler</span>
                    <span class="author-block" style="margin-right: 2rem;"><sup>2</sup>Ikerlan</span>
                    <span class="author-block"><sup>3</sup>Andreu World</span>
                  </div>

                  <div class="has-text-centered" style="font-size: 1.1rem; font-weight: bold; color: #000000;">
                    <p>British Machine Vision Conference (BMVC) 2025</p>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2506.09699" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                      <!-- Hugging Face link -->
                      <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-dark" disabled style="pointer-events: none; opacity: 0.6;">
                          <span class="icon">
                            <img src="https://huggingface.co/front/assets/huggingface_logo.svg" alt="Hugging Face" style="height:1em;vertical-align:middle;">
                          </span>
                          <span>Hugging Face (Coming Soon)</span>
                        </a>
                      </span>
                    </div>
                  </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="Teaser Image" style="width: 100vw; height: auto; display: block;">
      <div style="margin-top: 1rem;"></div>
      <h2 class="subtitle" style="text-align: justify;">
        The CHIP dataset features multi-sensor RGBD videos of a robotic arm manipulating wooden chairs in an industrial setting.
        The images are captured from multiple viewpoints and annotated with ground-truth 6D poses derived from the robot's kinematics, making it a valuable benchmark for evaluating 6D pose estimation methods in realistic scenarios. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurate 6D pose estimation of complex objects in 3D environments is essential for effective robotic manipulation.
            Yet, existing benchmarks fall short in evaluating 6D pose estimation methods under realistic industrial conditions as most datasets focus on household objects in domestic settings, while the few available industrial datasets are limited to artificial setups with objects placed on tables.
            To bridge this gap, we introduce CHIP, the first dataset designed for 6D pose estimation of chairs manipulated by a robotic arm in a real-world industrial environment.
            CHIP includes seven distinct chairs captured using three different RGBD sensing technologies and presents unique challenges, such as distractor objects with fine-grained differences and severe occlusions caused by the robotic arm and human operators.
            CHIP comprises 77,811 RGBD images annotated with ground-truth 6D poses automatically derived from the robot's kinematics, averaging 11,115 annotations per chair.
            We benchmark CHIP using three zero-shot 6D pose estimation methods, assessing performance across different sensor types, localization priors, and occlusion levels.
            Results show substantial room for improvement, highlighting the unique challenges posed by the dataset.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{nardon2025chip,
  title={CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings},
  author={Nardon, Mattia and Mujika Agirre, Mikel and González Tomé, Ander and Sedano Algarabel, Daniel and Rueda Collell, Josep and Caro, Ana Paola and Caraffa, Andrea and Poiesi, Fabio and Chippendale, Paul Ian and Boscaini, Davide},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2025}}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section class="section is light" id="acknowledgments">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      This work was supported by the European Union's Horizon Europe research and innovation programme under grant agreement No. 101058589 (AI-PRISM).
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <!-- Link to Paper -->
          <a class="icon-link" href="https://arxiv.org/pdf/2506.09699" target="_blank" title="View Paper on arXiv">
              <i class="fas fa-file-pdf fa-2x"></i>
          </a>
      </div>
      <div class="columns is-centered" style="margin-top: 20px;">
          <div class="column is-8">
              <div class="content has-text-centered">
                  <p>
                      This website is licensed under a
                      <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
                          Creative Commons Attribution-ShareAlike 4.0 International License
                      </a>.
                  </p>
                  <p>
                      Template adapted from
                      <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
                  </p>
              </div>
          </div>
      </div>
  </div>
</footer>


  </body>
  </html>
